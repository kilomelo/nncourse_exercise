{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# 设置参数\n",
    "trainSetPercent = 0.35\n",
    "num_iterations = 100\n",
    "learning_rate = 0.001\n",
    "loadLastTrainData = True\n",
    "sliceRandomSeed = 0\n",
    "useCupy = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data set count: 5996\n",
      "train set count: 2098\n",
      "test set count: 3898\n",
      "image size: 82944\n"
     ]
    }
   ],
   "source": [
    "# 准备数据\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "\n",
    "if useCupy:\n",
    "    import cupy as np\n",
    "    import numpy\n",
    "else:\n",
    "    import numpy as np\n",
    "\n",
    "labels= pd.read_csv('labels_final.csv')\n",
    "print('data set count:', labels.shape[0])\n",
    "\n",
    "\n",
    "trainSetCount = math.floor(labels.shape[0] * trainSetPercent)\n",
    "if trainSetCount < 1: trainSetCount = 1\n",
    "testSetCount = labels.shape[0] - trainSetCount\n",
    "\n",
    "print('train set count:', trainSetCount)\n",
    "print('test set count:', testSetCount)\n",
    "\n",
    "imgDir = './images/resized/'\n",
    "imgSize = 0\n",
    "\n",
    "np.random.seed = sliceRandomSeed\n",
    "shuffledDataSet = np.array(labels)\n",
    "np.random.shuffle(shuffledDataSet)\n",
    "trainSetRange = shuffledDataSet[:trainSetCount, :]\n",
    "testSetRange = shuffledDataSet[trainSetCount:trainSetCount + testSetCount, :]\n",
    "\n",
    "trainSetX = []\n",
    "\n",
    "for row in trainSetRange:\n",
    "    img = plt.imread(imgDir + str(row[0]) + '.jpg')\n",
    "    if imgSize == 0: imgSize = img.size\n",
    "    elif imgSize != img.size:\n",
    "        raise ValueError(\"图片尺寸不一致\")\n",
    "    imgArray = np.array(img)\n",
    "    imgTrans = imgArray.reshape((1, img.size)).T\n",
    "    trainSetX.append(imgTrans)\n",
    "\n",
    "print('image size:', imgSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造训练集\n",
    "trainSetX = np.array(trainSetX)\n",
    "trainSetX = trainSetX.squeeze().T / 2550\n",
    "trainSetY = trainSetRange[:,1:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters     \n",
    "\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p\n",
    "\n",
    "def print_mislabeled_images(classes, X, y, p):\n",
    "    \"\"\"\n",
    "    Plots images where predictions and truth were different.\n",
    "    X -- dataset\n",
    "    y -- true labels\n",
    "    p -- predictions\n",
    "    \"\"\"\n",
    "    a = p + y\n",
    "    mislabeled_indices = np.asarray(np.where(a == 1))\n",
    "    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n",
    "    num_images = len(mislabeled_indices[0])\n",
    "    for i in range(num_images):\n",
    "        index = mislabeled_indices[1][i]\n",
    "        \n",
    "        plt.subplot(2, num_images, i + 1)\n",
    "        plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Prediction: \" + classes[int(p[0,index])].decode(\"utf-8\") + \" \\n Class: \" + classes[y[0,index]].decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    assert(A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    # np.random.seed(1)\n",
    "    # parameters = {}\n",
    "    # L = len(layer_dims)\n",
    "    # print (layer_dims)\n",
    "    # for i in range(1, L):\n",
    "    #     #Be careful, the scaler is np.sqrt(layers_dims[i - 1]), not constant 0.01\n",
    "    #     parameters['W' + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) / np.sqrt(layer_dims[i - 1])\n",
    "    #     parameters[\"b\" + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "        \n",
    "    #     assert((layer_dims[i], layer_dims[i - 1]) == parameters[\"W\" + str(i)].shape)\n",
    "    #     assert((layer_dims[i], 1) == parameters[\"b\" + str(i)].shape)\n",
    "    \n",
    "    # return parameters\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    for i in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(i)], parameters[\"b\" + str(i)], \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_activation_forward(A, np.array(parameters[\"W\" + str(L)]), np.array(parameters[\"b\" + str(L)]), \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1, X.shape[1]))\n",
    "    \n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = -(np.dot(np.log(AL), Y.T) + np.dot(np.log(1 - AL), (1 - Y).T)) / (m * 1.0)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ( ))\n",
    "    return cost\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    current_cache = caches[L - 1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation=\"sigmoid\")\n",
    "    \n",
    "    for i in reversed(range(L - 1)):\n",
    "        current_cache = caches[i]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(i + 2)], current_cache, activation=\"relu\")\n",
    "        grads[\"dA\" + str(i + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(i + 1)] = dW_temp\n",
    "        grads[\"db\" + str(i + 1)] = db_temp\n",
    "        \n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    for i in range(1, L + 1):\n",
    "        parameters[\"W\" + str(i)] -= learning_rate * grads[\"dW\" + str(i)]\n",
    "        parameters[\"b\" + str(i)] -= learning_rate * grads[\"db\" + str(i)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, saved_parameters = None, cost_record_cnt = 10, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    if cost_record_cnt <= 0: cost_record_cnt = num_iterations\n",
    "    recordCostInterval = max(math.floor(num_iterations / cost_record_cnt), 1)\n",
    "\n",
    "    # np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    ### START CODE HERE ###\n",
    "    parameters = saved_parameters\n",
    "    if parameters == None:\n",
    "        parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ## START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        # if print_cost and i % 5 == 0:\n",
    "        #     print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        # if print_cost and i % 5 == 0:\n",
    "        #     costs.append(cost)\n",
    "        if i % recordCostInterval == 0:\n",
    "            costs.append(cost)\n",
    "            if print_cost:\n",
    "                print (\"Progress: %s cost: %.4f\" % (utils.strProgress(i, num_iterations, 20), cost))\n",
    "            \n",
    "    costs = np.array(costs)\n",
    "    # plot the cost\n",
    "    if useCupy:\n",
    "        plt.plot(numpy.squeeze(costs.get()))\n",
    "    else:\n",
    "        plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory used 38.755 GB of 63.732 GB | 61 %\n",
      "Progress: [....................] cost: 0.3495\n",
      "Progress: [#...................] cost: 0.3004\n",
      "Progress: [##..................] cost: 0.2962\n",
      "Progress: [###.................] cost: 0.2927\n",
      "Progress: [####................] cost: 0.2896\n",
      "Progress: [#####...............] cost: 0.2868\n",
      "Progress: [######..............] cost: 0.2841\n",
      "Progress: [#######.............] cost: 0.2817\n",
      "Progress: [########............] cost: 0.2793\n",
      "Progress: [#########...........] cost: 0.2770\n",
      "Progress: [##########..........] cost: 0.2748\n",
      "Progress: [###########.........] cost: 0.2726\n",
      "Progress: [############........] cost: 0.2704\n",
      "Progress: [#############.......] cost: 0.2683\n",
      "Progress: [##############......] cost: 0.2663\n",
      "Progress: [###############.....] cost: 0.2643\n",
      "Progress: [################....] cost: 0.2623\n",
      "Progress: [#################...] cost: 0.2604\n",
      "Progress: [##################..] cost: 0.2584\n",
      "Progress: [###################.] cost: 0.2565\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "memUsage = psutil.virtual_memory()\n",
    "print('memory used %.3f GB of %.3f GB | %.0f %%' % (memUsage.used / 1024 / 1024 / 1024, memUsage.total / 1024 / 1024 / 1024, memUsage.percent))\n",
    "\n",
    "layers_dims = [imgSize, 20, 7, 5, 1] #  5-layer model\n",
    "import utils\n",
    "if useCupy: saveDataFilePath = './saved_parameters_cupy.bin'\n",
    "else: saveDataFilePath = './saved_parameters.bin'\n",
    "saved_parameters = utils.loadFromeFile(saveDataFilePath)\n",
    "plt.rcParams['figure.figsize'] = (25.0, 4.0)\n",
    "parameters = L_layer_model(trainSetX, trainSetY, layers_dims, num_iterations = 1000, learning_rate = 0.0025, cost_record_cnt = 20, saved_parameters = saved_parameters, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to ./saved_parameters.bin done\n"
     ]
    }
   ],
   "source": [
    "# 保存模型\n",
    "\n",
    "utils.save2File(parameters, saveDataFilePath)\n",
    "print('save to', saveDataFilePath, 'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82944, 3898)\n",
      "(1, 3898)\n"
     ]
    }
   ],
   "source": [
    "# 构造测试集\n",
    "testSetX = []\n",
    "\n",
    "for row in testSetRange:\n",
    "    img = plt.imread(imgDir + str(row[0]) + '.jpg')\n",
    "    if imgSize != img.size:\n",
    "        raise ValueError(\"图片尺寸不一致\")\n",
    "    imgArray = np.array(img)\n",
    "    imgTrans = imgArray.reshape((1, img.size)).T\n",
    "    testSetX.append(imgTrans)\n",
    "\n",
    "testSetX = np.array(testSetX).squeeze().T / 2550\n",
    "testSetY = testSetRange[:,1:].T\n",
    "\n",
    "print(testSetX.shape)\n",
    "print(testSetY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3898)\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "Accuracy: 92.97 %\n"
     ]
    }
   ],
   "source": [
    "YHat, _ = L_model_forward(testSetX, parameters)\n",
    "print(YHat.shape)\n",
    "if useCupy:\n",
    "    result = np.array(numpy.vectorize(lambda x: 1 if x > 0.5 else 0)((YHat - testSetY).get()))\n",
    "else:\n",
    "    result = np.vectorize(lambda x: 1 if x > 0.5 else 0)(YHat - testSetY)\n",
    "print(result)\n",
    "goodCaseCnt = np.count_nonzero(result == 0)\n",
    "print('Accuracy: %.2f %%' % (100 * goodCaseCnt / testSetY.shape[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9588c58e218e3a9874d549bafbb26b5256434620a54f1bac9f7fe29051d12c0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
